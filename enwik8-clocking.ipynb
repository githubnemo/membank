{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import enwik8_data\n",
    "import imp\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import visdom\n",
    "vis = visdom.Visdom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = enwik8_data.hutter_raw_data(data_path='./data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA, VALID_DATA, TEST_DATA, unique_syms = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = len(unique_syms)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "enwik8_train_loader = DataLoader(TensorDataset(X_train, y_train),\n",
    "                               batch_size=64,\n",
    "                               shuffle=True)\n",
    "enwik8_valid_loader = DataLoader(TensorDataset(X_valid, y_valid),\n",
    "                               batch_size=64,\n",
    "                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(g):\n",
    "    for x, y in g:\n",
    "        yield torch.from_numpy(x).long(), torch.from_numpy(y).long()\n",
    "\n",
    "class Enwik8TrainLoader:\n",
    "    def __init__(self, _dataset, batch_size=128, num_steps=32, **kwargs):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "    def __iter__(self):\n",
    "        return collate(enwik8_data.data_iterator(TRAIN_DATA, self.batch_size, self.num_steps))\n",
    "\n",
    "class Enwik8ValidLoader:\n",
    "    def __init__(self, _dataset, batch_size=128, num_steps=32, **kwargs):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "    def __iter__(self):\n",
    "        return collate(enwik8_data.data_iterator(VALID_DATA, self.batch_size, self.num_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_flatten(t):\n",
    "    return t.view(t.size(0) * t.size(1), -1)\n",
    "\n",
    "def time_unflatten(t, s):\n",
    "    return t.view(s[0], s[1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReconModel(nn.Module):\n",
    "    def __init__(self, num_hidden=64, num_modules=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(EMBEDDING_SIZE, num_hidden)\n",
    "        self.rnn = models.ClockingCWRNN(num_hidden, num_hidden, num_modules)\n",
    "        self.clf = nn.Linear(num_hidden, EMBEDDING_SIZE)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_emb = self.emb(x.long())\n",
    "        l0, h0 = self.rnn(x_emb)\n",
    "        \n",
    "        vis.heatmap(l0[0].data.numpy(), win=\"act\")\n",
    "        vis.heatmap(self.rnn.module_periods.data.numpy().reshape(1, -1), win=\"periods\")\n",
    "        vis.heatmap(self.rnn.module_shifts.data.numpy().reshape(1, -1), win=\"shifts\")\n",
    "\n",
    "        l1 = self.clf(time_flatten(l0))\n",
    "        l1_sm = self.softmax(l1)\n",
    "        \n",
    "        return time_unflatten(l1_sm, x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Trainer(skorch.NeuralNet):\n",
    "    def __init__(self, \n",
    "                 criterion=nn.NLLLoss,\n",
    "                 *args, \n",
    "                 **kwargs):\n",
    "        super().__init__(*args, criterion=criterion, **kwargs)\n",
    "\n",
    "    def get_loss(self, y_pred, y_true, X=None, train=False):\n",
    "        pred = time_flatten(y_pred)\n",
    "        true = time_flatten(y_true).squeeze(-1)\n",
    "        return super().get_loss(pred, true, X=X, train=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "class BatchPrinter(skorch.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.batches_per_epoch = None\n",
    "        self.batch_counter = 0\n",
    "    def on_batch_begin(self, *args, **kwargs):\n",
    "        self.batch_start_time = time.time()\n",
    "    def on_batch_end(self, *args, **kwargs):\n",
    "        self.batch_end_time = time.time()\n",
    "        self.batch_counter += 1\n",
    "        sys.stdout.write(\"Batch {}/{} complete ({:.2}s).\\r\".format(\n",
    "            self.batch_counter, \n",
    "            self.batches_per_epoch,\n",
    "            self.batch_end_time - self.batch_start_time,\n",
    "        ))\n",
    "        sys.stdout.flush()\n",
    "    def on_epoch_end(self, *args, **kwargs):\n",
    "        if self.batches_per_epoch is None:\n",
    "            self.batches_per_epoch = self.batch_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "ef = Trainer(module=ReconModel,\n",
    "             optim=torch.optim.Adam,\n",
    "             lr=0.005,\n",
    "             max_epochs=60,\n",
    "                  \n",
    "             train_split=None,\n",
    "             iterator_train=Enwik8TrainLoader,\n",
    "             iterator_train__batch_size=32,\n",
    "             iterator_train__num_steps=32,\n",
    "             iterator_test=Enwik8ValidLoader,\n",
    "             iterator_test__batch_size=32,\n",
    "             iterator_test__num_steps=32,\n",
    "             \n",
    "             module__num_modules=8,\n",
    "             module__num_hidden=64,\n",
    "             \n",
    "             callbacks=[BatchPrinter()]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n",
      "  epoch    train_loss        dur)..\n",
      "-------  ------------  ---------\n",
      "      1        \u001b[36m2.3305\u001b[0m  9805.0095\n",
      "      2        \u001b[36m2.2666\u001b[0m  9877.3649\n",
      "      3        \u001b[36m2.2161\u001b[0m  9878.4157\n",
      "Batch 274438/87890 complete (0.1s).).\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Trainer at 0x7f4ac588fe48>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pdb on\n",
    "ef.fit(torch.zeros((10,1)), torch.zeros((10,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
